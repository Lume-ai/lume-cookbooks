{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstraction: How to auto-map data with 2 lines of code\n",
    "\n",
    "This cookbook builds upon the foundation of the `Use Existing Pipeline` cookbook, linked [here](https://github.com/Lume-ai/lume-cookbooks/tree/main/examples/use_existing_pipeline). This contains the same functionality, but provides abstracted functions to make it a few-line integration. \n",
    "\n",
    "‚ùì See a video walkthrough of this notebook [here](https://www.loom.com/share/63a42b2f4b6d4439a45e461ea543033c)\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook contains the following 1 section:\n",
    "\n",
    "- **Map incoming source data using an existing pipeline:** Specify a set of functions and use the Lume API to map data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map incoming source data using an existing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your API key here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '<YOUR_API_KEY>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "First let's define a few utilities for making calls to the Lume API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx \n",
    "import traceback\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "url = \"https://api.lume.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_pipeline(pipeline_id):\n",
    "    new_url = f'{url}/pipelines/{pipeline_id}'\n",
    "    headers = {\"lume-api-key\": api_key}\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        job = await client.get(new_url, headers=headers)\n",
    "        job = job.json()\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_job(pipeline_id, data):\n",
    "    new_url = f'{url}/pipelines/{pipeline_id}/jobs'\n",
    "    headers = {\"lume-api-key\": api_key}\n",
    "    payload = {\n",
    "        \"data\": data\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        job = await client.post(new_url, headers=headers, json=payload)\n",
    "        job = job.json()\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_job(job_id):\n",
    "    new_url = f'{url}/jobs/{job_id}/run'\n",
    "    headers = {\"lume-api-key\": api_key}\n",
    "    payload = {\n",
    "        \"immediate_return\": True # required to set this to True for polling.\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=6000) as client:\n",
    "        job = await client.post(new_url, headers=headers, json=payload)\n",
    "        job = job.json()\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_result(result_id):\n",
    "    new_url = f'{url}/results/{result_id}'\n",
    "    headers = {\"lume-api-key\": api_key}\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        job = await client.get(new_url, headers=headers)\n",
    "        job = job.json()\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def poll_result(result_id, interval=3):\n",
    "    while True:\n",
    "        result = await get_result(result_id)\n",
    "        if result['status'] != 'running':\n",
    "            return result\n",
    "        await asyncio.sleep(interval)  # Wait for the specified interval before polling again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_mappings_from_result(result_id, page=1, size=50):\n",
    "    new_url = f'{url}/results/{result_id}/mappings'\n",
    "    headers = {\"lume-api-key\": api_key}\n",
    "    params = {\n",
    "        'page': page,  # Assuming you want to access the second page\n",
    "        'size': size  # Number of records per page\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        job = await client.get(new_url, headers=headers, params=params)\n",
    "        job = job.json()\n",
    "    return job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to iterate over all pages of pipelines to get all pipelines\n",
    "async def get_all_pipelines():\n",
    "    new_url = f'{url}/pipelines' \n",
    "    headers = {\"lume-api-key\": api_key} \n",
    "    all_pipelines = []\n",
    "    page = 1\n",
    "    total_pages = None\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        while total_pages is None or page <= total_pages:\n",
    "            response = await client.get(f\"{new_url}?page={page}\", headers=headers)\n",
    "            data = response.json()\n",
    "            all_pipelines.extend(data['items'])\n",
    "            if total_pages is None:\n",
    "                total_items = data['total']\n",
    "                page_size = data['size']\n",
    "                total_pages = (total_items + page_size - 1) // page_size  # Calculate total pages\n",
    "            page += 1\n",
    "\n",
    "    return all_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_pipeline_with_name(pipeline_name):\n",
    "    pipelines = await get_all_pipelines()\n",
    "    for pipeline in pipelines:\n",
    "        if pipeline['name'] == pipeline_name:\n",
    "            return pipeline\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_mappings(result_id):\n",
    "    mappings = []\n",
    "    first_page = await get_mappings_from_result(result_id)\n",
    "    mappings.extend(first_page['items'])\n",
    "\n",
    "    total_items = first_page['total']\n",
    "    page_size = first_page['size']\n",
    "    total_pages = total_items // page_size + 1\n",
    "\n",
    "    for page in range(2, total_pages + 1):\n",
    "        new_mappings_page = await get_mappings_from_result(result_id, page=page)\n",
    "        mappings.extend(new_mappings_page['items'])\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def executeJobTransformation(pipeline_name, data):\n",
    "    pipeline = await get_pipeline_with_name(pipeline_name)\n",
    "    if pipeline is None:\n",
    "        raise ValueError(f\"Pipeline with name {pipeline_name} not found\")\n",
    "    job = await create_job(pipeline['id'], data)\n",
    "    initial_result = await run_job(job['id'])\n",
    "    result = await poll_result(initial_result['id'])\n",
    "    all_mappings = await get_all_mappings(result['id'])\n",
    "    return all_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Context\n",
    "\n",
    "This cookbook assumes a pipeline has already been created, called `ecomm_test`. The existing pipeline is meant to map source ecommerce data to an internal ecommerce data model. The target schema used in the pipeline is in this cookbook's folder, as `target_schema.json`. The cell below loads the target schema. You can view it in detail in taret_schema.json within this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_schema_path = os.path.join(os.getcwd(), 'target_schema.json')\n",
    "with open(target_schema_path) as f:\n",
    "    target_schema = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Let's access our source data and use a Lume pipeline to map it automatically.\n",
    "\n",
    "The source data is in this cookbook's folder, as `source_data.json`. The cell below loads the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_path = os.path.join(os.getcwd(), 'source_data.json')\n",
    "with open(source_data_path) as f:\n",
    "    source_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use Lume to map this source data automatically, using an existing pipeline. Use the abstracted `executeJobTransformation` function to do so in 2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Use the abstracted `executeJobTransformation` function to map data with 2 lines of code\n",
    "Depending on where your source data arrived (system x, api y, etc), use that knowledge to fetch the corresponding pipeline via the pipeline name, `ecommerce_demo` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'ecommerce-demo'\n",
    "all_mappings = executeJobTransformation(pipeline_name, source_data)\n",
    "all_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Pipe the output to your end destination\n",
    "After getting the final mapped data, send it to the next step of your workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
